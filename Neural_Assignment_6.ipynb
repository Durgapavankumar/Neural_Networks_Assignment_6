{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#Durga Pavan Kumar Pailla\n",
        "#700758689"
      ],
      "metadata": {
        "id": "-aHyWD6_nfYj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd # Basic packages for creating dataframes and loading dataset\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt # Package for visualization\n",
        "import re # importing package for Regular expression operations\n",
        "from sklearn.model_selection import train_test_split # Package for splitting the data\n",
        "from sklearn.preprocessing import LabelEncoder # Package for conversion of categorical to Numerical\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer # Tokenization\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences # Add zeros or crop based on the length\n",
        "from tensorflow.keras.models import Sequential # Sequential Neural Network\n",
        "from tensorflow.keras.layers import Dense, Embedding, LSTM, SpatialDropout1D # For layers in Neural Network\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "# Load the dataset as a Pandas DataFrame\n",
        "path_to_csv = 'Sentiment.csv'\n",
        "dataset = pd.read_csv(path_to_csv, header=0)\n",
        "\n",
        "# Select only the necessary columns 'text' and 'sentiment'\n",
        "mask = dataset.columns.isin(['text', 'sentiment'])\n",
        "data = dataset.loc[:, mask]\n",
        "\n",
        "# Keeping only the necessary columns\n",
        "data['text'] = data['text'].apply(lambda x: x.lower())\n",
        "data['text'] = data['text'].apply(lambda x: re.sub('[^a-zA-Z0-9\\s]', '', x))\n",
        "\n",
        "for idx, row in data.iterrows():\n",
        "    row[0] = row[0].replace('rt', ' ') # Removing Retweets\n",
        "\n",
        "max_features = 2000\n",
        "tokenizer = Tokenizer(num_words=max_features, split=' ') # Maximum words is 2000 to tokenize sentence\n",
        "tokenizer.fit_on_texts(data['text'].values)\n",
        "X = tokenizer.texts_to_sequences(data['text'].values) # Taking values to feature matrix\n",
        "X = pad_sequences(X) # Padding the feature matrix\n",
        "\n",
        "embed_dim = 128 # Dimension of the Embedded layer\n",
        "lstm_out = 196 # Long short-term memory (LSTM) layer neurons\n",
        "\n",
        "def createmodel():\n",
        "    model = Sequential() # Sequential Neural Network\n",
        "    model.add(Embedding(max_features, embed_dim, input_length = X.shape[1])) # input dimension 2000 Neurons, output dimension 128 Neurons\n",
        "    model.add(LSTM(lstm_out, dropout=0.2, recurrent_dropout=0.2)) # Drop out 20%, 196 output Neurons, recurrent dropout 20%\n",
        "    model.add(Dense(3, activation='softmax')) # 3 output neurons[positive, Neutral, Negative], softmax as activation\n",
        "    model.compile(loss = 'categorical_crossentropy', optimizer='adam', metrics = ['accuracy']) # Compiling the model\n",
        "    return model\n",
        "\n",
        "labelencoder = LabelEncoder() # Applying label Encoding on the label matrix\n",
        "integer_encoded = labelencoder.fit_transform(data['sentiment']) # Fitting the model\n",
        "y = to_categorical(integer_encoded)\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, y, test_size=0.33, random_state=42) # 67% training data, 33% test data split\n",
        "\n",
        "batch_size = 32 # Batch size 32\n",
        "model = createmodel() # Function call to Sequential Neural Network\n",
        "model.fit(X_train, Y_train, epochs=1, batch_size=batch_size, verbose=2) # verbose the higher, the more messages\n",
        "score, acc = model.evaluate(X_test, Y_test, verbose=2, batch_size=batch_size) # evaluating the model\n",
        "print(score)\n",
        "print(acc)\n",
        "print(model.metrics_names) # metrics of the model\n",
        "print(integer_encoded)\n",
        "print(data['sentiment'])\n",
        "\n",
        "# Predicting on the text data\n",
        "sentence = ['A lot of good things are happening. We are respected again throughout the world, and that is a great thing.@realDonaldTrump']\n",
        "sentence = tokenizer.texts_to_sequences(sentence) # Tokenizing the sentence\n",
        "sentence = pad_sequences(sentence, maxlen=X.shape[1], dtype='int32', value=0) # Padding the sentence\n",
        "sentiment_probs = model.predict(sentence, batch_size=1, verbose=2)[0] # Predicting the sentence text\n",
        "sentiment = np.argmax(sentiment_probs)\n",
        "\n",
        "print(sentiment_probs)\n",
        "if sentiment == 0:\n",
        "    print(\"Neutral\")\n",
        "elif sentiment == 1:\n",
        "    print(\"Negative\")\n",
        "else:\n",
        "    print(\"Positive\")\n",
        "\n",
        "# Custom wrapper for Keras model\n",
        "from sklearn.base import BaseEstimator, ClassifierMixin\n",
        "\n",
        "class CustomKerasClassifier(BaseEstimator, ClassifierMixin):\n",
        "    def __init__(self, build_fn=None, epochs=1, batch_size=32, verbose=1, **sk_params):\n",
        "        self.build_fn = build_fn\n",
        "        self.epochs = epochs\n",
        "        self.batch_size = batch_size\n",
        "        self.verbose = verbose\n",
        "        self.sk_params = sk_params\n",
        "        self.model = None\n",
        "\n",
        "    def fit(self, X, y, **kwargs):\n",
        "        self.model = self.build_fn()\n",
        "        return self.model.fit(X, y, epochs=self.epochs, batch_size=self.batch_size, verbose=self.verbose, **kwargs)\n",
        "\n",
        "    def predict(self, X, **kwargs):\n",
        "        return self.model.predict(X, **kwargs)\n",
        "\n",
        "    def predict_proba(self, X, **kwargs):\n",
        "        return self.model.predict(X, **kwargs)\n",
        "\n",
        "    def score(self, X, y, **kwargs):\n",
        "        _, accuracy = self.model.evaluate(X, y, verbose=0)\n",
        "        return accuracy\n",
        "\n",
        "# Use the custom Keras classifier\n",
        "model = CustomKerasClassifier(build_fn=createmodel, verbose=2)\n",
        "batch_size = [10, 20, 40]\n",
        "epochs = [1, 2]\n",
        "param_grid = {'batch_size': batch_size, 'epochs': epochs}\n",
        "grid = GridSearchCV(estimator=model, param_grid=param_grid)\n",
        "grid_result = grid.fit(X_train, Y_train)\n",
        "\n",
        "# Summarize results\n",
        "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "caJyiQ1P6O_2",
        "outputId": "b13fb345-954c-44a5-bd5c-633f3ce5c4c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-10-6d10c9e89d21>:23: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  data['text'] = data['text'].apply(lambda x: x.lower())\n",
            "<ipython-input-10-6d10c9e89d21>:24: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  data['text'] = data['text'].apply(lambda x: re.sub('[^a-zA-Z0-9\\s]', '', x))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "291/291 - 46s - loss: 0.8224 - accuracy: 0.6441 - 46s/epoch - 159ms/step\n",
            "144/144 - 3s - loss: 0.7604 - accuracy: 0.6654 - 3s/epoch - 20ms/step\n",
            "0.7604355216026306\n",
            "0.6653560400009155\n",
            "['loss', 'accuracy']\n",
            "[1 2 1 ... 2 0 2]\n",
            "0         Neutral\n",
            "1        Positive\n",
            "2         Neutral\n",
            "3        Positive\n",
            "4        Positive\n",
            "           ...   \n",
            "13866    Negative\n",
            "13867    Positive\n",
            "13868    Positive\n",
            "13869    Negative\n",
            "13870    Positive\n",
            "Name: sentiment, Length: 13871, dtype: object\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:5 out of the last 5 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7ad0db684310> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 - 0s - 259ms/epoch - 259ms/step\n",
            "[0.6961828  0.14821675 0.1556004 ]\n",
            "Neutral\n",
            "744/744 - 93s - loss: 0.8199 - accuracy: 0.6455 - 93s/epoch - 125ms/step\n",
            "744/744 - 97s - loss: 0.8228 - accuracy: 0.6421 - 97s/epoch - 130ms/step\n",
            "744/744 - 93s - loss: 0.8218 - accuracy: 0.6493 - 93s/epoch - 125ms/step\n",
            "744/744 - 94s - loss: 0.8286 - accuracy: 0.6414 - 94s/epoch - 126ms/step\n",
            "744/744 - 94s - loss: 0.8147 - accuracy: 0.6502 - 94s/epoch - 126ms/step\n",
            "Epoch 1/2\n",
            "744/744 - 98s - loss: 0.8284 - accuracy: 0.6457 - 98s/epoch - 132ms/step\n",
            "Epoch 2/2\n",
            "744/744 - 92s - loss: 0.6847 - accuracy: 0.7113 - 92s/epoch - 123ms/step\n",
            "Epoch 1/2\n",
            "744/744 - 94s - loss: 0.8254 - accuracy: 0.6421 - 94s/epoch - 126ms/step\n",
            "Epoch 2/2\n",
            "744/744 - 91s - loss: 0.6763 - accuracy: 0.7112 - 91s/epoch - 122ms/step\n",
            "Epoch 1/2\n",
            "744/744 - 89s - loss: 0.8237 - accuracy: 0.6450 - 89s/epoch - 120ms/step\n",
            "Epoch 2/2\n",
            "744/744 - 90s - loss: 0.6755 - accuracy: 0.7116 - 90s/epoch - 121ms/step\n",
            "Epoch 1/2\n",
            "744/744 - 97s - loss: 0.8239 - accuracy: 0.6430 - 97s/epoch - 131ms/step\n",
            "Epoch 2/2\n",
            "744/744 - 93s - loss: 0.6737 - accuracy: 0.7170 - 93s/epoch - 126ms/step\n",
            "Epoch 1/2\n",
            "744/744 - 91s - loss: 0.8178 - accuracy: 0.6487 - 91s/epoch - 122ms/step\n",
            "Epoch 2/2\n",
            "744/744 - 91s - loss: 0.6657 - accuracy: 0.7185 - 91s/epoch - 122ms/step\n",
            "372/372 - 54s - loss: 0.8349 - accuracy: 0.6426 - 54s/epoch - 145ms/step\n",
            "372/372 - 55s - loss: 0.8274 - accuracy: 0.6411 - 55s/epoch - 147ms/step\n",
            "372/372 - 55s - loss: 0.8271 - accuracy: 0.6464 - 55s/epoch - 147ms/step\n",
            "372/372 - 51s - loss: 0.8333 - accuracy: 0.6397 - 51s/epoch - 138ms/step\n",
            "372/372 - 57s - loss: 0.8265 - accuracy: 0.6418 - 57s/epoch - 153ms/step\n",
            "Epoch 1/2\n",
            "372/372 - 54s - loss: 0.8348 - accuracy: 0.6473 - 54s/epoch - 145ms/step\n",
            "Epoch 2/2\n",
            "372/372 - 53s - loss: 0.6786 - accuracy: 0.7147 - 53s/epoch - 141ms/step\n",
            "Epoch 1/2\n",
            "372/372 - 51s - loss: 0.8221 - accuracy: 0.6481 - 51s/epoch - 136ms/step\n",
            "Epoch 2/2\n",
            "372/372 - 51s - loss: 0.6802 - accuracy: 0.7132 - 51s/epoch - 136ms/step\n",
            "Epoch 1/2\n",
            "372/372 - 55s - loss: 0.8315 - accuracy: 0.6387 - 55s/epoch - 147ms/step\n",
            "Epoch 2/2\n",
            "372/372 - 52s - loss: 0.6815 - accuracy: 0.7151 - 52s/epoch - 140ms/step\n",
            "Epoch 1/2\n",
            "372/372 - 57s - loss: 0.8280 - accuracy: 0.6397 - 57s/epoch - 152ms/step\n",
            "Epoch 2/2\n",
            "372/372 - 51s - loss: 0.6709 - accuracy: 0.7130 - 51s/epoch - 138ms/step\n",
            "Epoch 1/2\n",
            "372/372 - 53s - loss: 0.8381 - accuracy: 0.6385 - 53s/epoch - 144ms/step\n",
            "Epoch 2/2\n",
            "372/372 - 49s - loss: 0.6746 - accuracy: 0.7116 - 49s/epoch - 132ms/step\n",
            "186/186 - 33s - loss: 0.8493 - accuracy: 0.6321 - 33s/epoch - 177ms/step\n",
            "186/186 - 35s - loss: 0.8428 - accuracy: 0.6367 - 35s/epoch - 186ms/step\n",
            "186/186 - 36s - loss: 0.8437 - accuracy: 0.6357 - 36s/epoch - 192ms/step\n",
            "186/186 - 33s - loss: 0.8503 - accuracy: 0.6351 - 33s/epoch - 178ms/step\n",
            "186/186 - 35s - loss: 0.8431 - accuracy: 0.6366 - 35s/epoch - 187ms/step\n",
            "Epoch 1/2\n",
            "186/186 - 35s - loss: 0.8579 - accuracy: 0.6297 - 35s/epoch - 186ms/step\n",
            "Epoch 2/2\n",
            "186/186 - 31s - loss: 0.6950 - accuracy: 0.7006 - 31s/epoch - 167ms/step\n",
            "Epoch 1/2\n",
            "186/186 - 33s - loss: 0.8357 - accuracy: 0.6404 - 33s/epoch - 176ms/step\n",
            "Epoch 2/2\n",
            "186/186 - 31s - loss: 0.6883 - accuracy: 0.7062 - 31s/epoch - 168ms/step\n",
            "Epoch 1/2\n",
            "186/186 - 35s - loss: 0.8452 - accuracy: 0.6321 - 35s/epoch - 187ms/step\n",
            "Epoch 2/2\n",
            "186/186 - 30s - loss: 0.6822 - accuracy: 0.7117 - 30s/epoch - 162ms/step\n",
            "Epoch 1/2\n",
            "186/186 - 37s - loss: 0.8445 - accuracy: 0.6344 - 37s/epoch - 196ms/step\n",
            "Epoch 2/2\n",
            "186/186 - 30s - loss: 0.6826 - accuracy: 0.7061 - 30s/epoch - 164ms/step\n",
            "Epoch 1/2\n",
            "186/186 - 33s - loss: 0.8384 - accuracy: 0.6354 - 33s/epoch - 178ms/step\n",
            "Epoch 2/2\n",
            "186/186 - 31s - loss: 0.6760 - accuracy: 0.7176 - 31s/epoch - 165ms/step\n",
            "Epoch 1/2\n",
            "233/233 - 45s - loss: 0.8322 - accuracy: 0.6394 - 45s/epoch - 194ms/step\n",
            "Epoch 2/2\n",
            "233/233 - 39s - loss: 0.6823 - accuracy: 0.7097 - 39s/epoch - 169ms/step\n",
            "Best: 0.678682 using {'batch_size': 40, 'epochs': 2}\n"
          ]
        }
      ]
    }
  ]
}